{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle #for reading the given files\n",
    "import numpy as np \n",
    "import cv2 #to read images\n",
    "import matplotlib.pyplot as plt #to show images and graphs\n",
    "\n",
    "#importing for our model\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D, Flatten,MaxPooling2D,Dropout,Activation,BatchNormalization\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Opening the pickle files provided and storing them in lists\n",
    "with open('train_image.pkl', 'rb') as f:\n",
    "    train = pickle.load(f) #train images\n",
    "with open('test_image.pkl', 'rb') as f:\n",
    "    test = pickle.load(f) #test images\n",
    "with open('train_label.pkl', 'rb') as f:\n",
    "    train_labels = pickle.load(f) #train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting to numpy nd array\n",
    "train_np = np.array(train)\n",
    "test_np = np.array(test)\n",
    "train_labels_np = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 784), (2000, 784), (8000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np.shape,test_np.shape,train_labels_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reshaping the images in appropriate size (-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_img = train_np.reshape(-1,28,28,1)\n",
    "test_img = test_np.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train_img.astype('float32')\n",
    "X_test = test_img.astype('float32')\n",
    "#dividing the image by 255\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the labels from 0,2,3,6 to 0,1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = train_labels_np\n",
    "for i in range(len(y_train)):\n",
    "    if(y_train[i] == 0):\n",
    "        y_train[i] = 0\n",
    "    elif(y_train[i] == 2):\n",
    "        y_train[i] = 1\n",
    "    elif(y_train[i] == 3):\n",
    "        y_train[i] = 2\n",
    "    else:\n",
    "        y_train[i] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "#### Why Data Augmentation?\n",
    "Here we are provided a very small dataset. So our model will not be able to learn properly and thus result in very low test accuracy even though the train accuracy may me high(over fitting).\n",
    "\n",
    "So, to improve the learning of our model, we need to feed it more data. We can do so by altering the data that is already provided to us.\n",
    "\n",
    "There are various data augmentation techniques like **Flipping the image**, **rotating the image**, **changing the image's orientation, cropping, translation,etc**\n",
    "\n",
    "Here we are going to use 2 of the above techniqes:\n",
    "1) Rotation\n",
    "2) Flipping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Rotation\n",
    "library used: numpy\n",
    "\n",
    "code : **np.rot90(img,k)**, where k = number of 90 degree rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n"
     ]
    }
   ],
   "source": [
    "x_t = list(X_train) \n",
    "y_t = list(y_train)\n",
    "x_t_len = len(x_t)\n",
    "for i in range(x_t_len):\n",
    "    img = x_t[i] \n",
    "    rot = np.rot90(img,1) #rotating the image\n",
    "    x_t.append(rot) #appending flipped image to the list\n",
    "    y_t.append(y_t[i]) #adding the image label\n",
    "    \n",
    "print(len(x_t))\n",
    "X_train = np.array(x_t) #converting to numpy\n",
    "y_train = np.array(y_t) #converting to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJYAAACTCAYAAABszOBRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d2ff8762b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAJYAAACTCAYAAABszOBRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d2ffc30be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking whether the rotation is successful or not\n",
    "\n",
    "img = X_train[12].reshape(28,28)\n",
    "img1 = X_train[8012].reshape(28,28)\n",
    "flip = np.fliplr(img)\n",
    "plt.subplot(2,1,1)\n",
    "plt.imshow(img)\n",
    "plt.xlabel('not rotated')\n",
    "plt.ylabel(y_train[12])\n",
    "plt.show()\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(img1)\n",
    "plt.xlabel(\"rotated\")\n",
    "plt.ylabel(y_train[8012])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Flipping\n",
    "\n",
    "library used: numpy\n",
    "\n",
    "code: **np.fliplr(img)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32000\n"
     ]
    }
   ],
   "source": [
    "x_t = list(X_train)\n",
    "y_t = list(y_train)\n",
    "x_t_len = len(x_t)\n",
    "for i in range(x_t_len):\n",
    "    img = x_t[i]\n",
    "    flip = np.fliplr(img) #flipping the image\n",
    "    x_t.append(flip) #appending flipped image to the list\n",
    "    y_t.append(y_t[i]) #adding the image label\n",
    "    \n",
    "print(len(x_t))\n",
    "X_train = np.array(x_t) #converting to np array\n",
    "y_train = np.array(y_t) #converting to np array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32000, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one-hot encoding the labels\n",
    "from keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27200, 28, 28, 1), (4800, 28, 28, 1), (27200, 4), (4800, 4))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,Y_train,Y_val = train_test_split(X_train,y_train,test_size=0.15)\n",
    "X_train.shape,X_val.shape,Y_train.shape,Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "## Library used to create model: Keras\n",
    "\n",
    "We are going to create a deep learning model. We are going to use **Convolutional Neural Network(CNN)** as they are highly effective on images and give excellent results on image classification\n",
    "\n",
    "## Our architecture: \n",
    "<ul><li> input image: 28x28x1 image</li></ul>\n",
    "<ol>\n",
    "<li>**Convolution layer**: filter: 3*3, units: 64, strides=1,padding: SAME (i.e image size remains same) </li>\n",
    "<ul><li> output of this layer: 28x28x64</li></ul><br/>\n",
    "<li>**Batch normalization**: for normalizing the output</li>\n",
    "<li>**Maxpool layer**: filter:2x2,strides=2</li>\n",
    "<ul><li> output of this layer: 14x14x64</li></ul><br/>\n",
    "<li>**Convolution layer**: filter: 3*3, units: 128, padding: SAME (i.e image size remains same)</li>\n",
    "<li>**Batch normalization**: for normalizing the output</li>\n",
    "<ul><li> output of this layer: 14x14x128</li></ul><br/>\n",
    "<li>**Maxpool layer**: filter:2x2,strides=2</li>\n",
    "<ul><li> output of this layer: 7x7x128</li></ul><br/>\n",
    "<li>**Dropout layer**: FOR REGULARIZATION(i.e prevent overfitting)</li>\n",
    "\n",
    "<li>**Convolution layer**: filter: 3*3, units: 256, padding: SAME (i.e image size remains same)</li>\n",
    "<li>**Batch normalization**: for normalizing the output</li>\n",
    "<ul><li> output of this layer: 7x7x128</li></ul><br/>\n",
    "<li>**Maxpool layer**: filter:2x2,strides=2</li>\n",
    "<ul><li> output of this layer: 3x3x128</li></ul><br/>\n",
    "\n",
    "<li>**Flatten**: Now we flatten the array to pass it through the neural network</li>\n",
    "<li>**Dense layer**: 1024 units, activation: relu</li>\n",
    "<li>**Output layer**: units: 4,activation: softmax</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(64,(3,3),padding='same',activation='relu',input_shape = (28,28,1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=None,padding='valid',data_format=None))\n",
    "\n",
    "model.add(Conv2D(128,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=None,padding='valid',data_format=None))\n",
    "model.add(Dropout(0.2, noise_shape=None, seed=None))\n",
    "\n",
    "model.add(Conv2D(256,(3,3),padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2),strides=None,padding='valid',data_format=None))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024,activation='relu'))\n",
    "model.add(Dense(4,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 7, 7, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 7, 7, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              2360320   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 4100      \n",
      "=================================================================\n",
      "Total params: 2,735,876\n",
      "Trainable params: 2,734,980\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27200 samples, validate on 4800 samples\n",
      "Epoch 1/10\n",
      "27200/27200 [==============================] - 274s 10ms/step - loss: 0.8656 - acc: 0.7618 - val_loss: 0.5132 - val_acc: 0.7973\n",
      "Epoch 2/10\n",
      "27200/27200 [==============================] - 288s 11ms/step - loss: 0.3943 - acc: 0.8425 - val_loss: 0.3961 - val_acc: 0.8490\n",
      "Epoch 3/10\n",
      "27200/27200 [==============================] - 268s 10ms/step - loss: 0.3348 - acc: 0.8679 - val_loss: 0.3480 - val_acc: 0.8665\n",
      "Epoch 4/10\n",
      "27200/27200 [==============================] - 275s 10ms/step - loss: 0.2989 - acc: 0.8835 - val_loss: 0.3548 - val_acc: 0.8627\n",
      "Epoch 5/10\n",
      "27200/27200 [==============================] - 267s 10ms/step - loss: 0.2584 - acc: 0.8993 - val_loss: 0.3909 - val_acc: 0.8638\n",
      "Epoch 6/10\n",
      "27200/27200 [==============================] - 271s 10ms/step - loss: 0.2336 - acc: 0.9097 - val_loss: 0.3948 - val_acc: 0.8696\n",
      "Epoch 7/10\n",
      "27200/27200 [==============================] - 273s 10ms/step - loss: 0.2037 - acc: 0.9214 - val_loss: 0.3490 - val_acc: 0.8831\n",
      "Epoch 8/10\n",
      "27200/27200 [==============================] - 276s 10ms/step - loss: 0.1772 - acc: 0.9314 - val_loss: 0.3643 - val_acc: 0.8827\n",
      "Epoch 9/10\n",
      "27200/27200 [==============================] - 275s 10ms/step - loss: 0.1522 - acc: 0.9417 - val_loss: 0.3622 - val_acc: 0.8848\n",
      "Epoch 10/10\n",
      "27200/27200 [==============================] - 270s 10ms/step - loss: 0.1383 - acc: 0.9474 - val_loss: 0.4161 - val_acc: 0.8675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d288204588>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(X_train, Y_train,validation_data=(X_val,Y_val), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 28, 28, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000,), {0, 1, 2, 3})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#choosing the max argument\n",
    "y_pred = np.argmax(predictions,axis=1)\n",
    "y_pred.shape,set(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the classes back to their original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(y_pred.shape[0]):\n",
    "    if(y_pred[i] == 0):\n",
    "        y_pred[i] = 0\n",
    "    elif(y_pred[i] == 1):\n",
    "        y_pred[i] = 2\n",
    "    elif(y_pred[i] == 2):\n",
    "        y_pred[i] = 3\n",
    "    else:\n",
    "        y_pred[i] = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ans = np.zeros((y_pred.shape[0],2))\n",
    "for i in range(y_pred.shape[0]):\n",
    "    ans[i][0] = i\n",
    "    ans[i][1] = y_pred[i]\n",
    "pd.DataFrame(ans).to_csv('Vibhu_Sehra.csv',header = ['image_index','class'],index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
